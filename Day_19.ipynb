{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE:__ For transforming the bboxes by albumentations library make sure you convert the bbox into 'coco' format which is [x_min, y_min, width, height], since it is the most stable. On repeated experimentation it was found that 'yolo' and 'pascal_voc' seems to be broken."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "import selectivesearch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_aug_p = 0.5\n",
    "iou_threshold = 0.4\n",
    "img_crop_size = (224, 224)\n",
    "img_resize_size = 256\n",
    "\n",
    "# mean and std-dev taken from imagenet dataset\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://albumentations.ai/docs/examples/example_bboxes/\n",
    "def draw_bounding_boxes(image, bbox, category):\n",
    "    x_1, y_1, x_2, y_2 = bbox\n",
    "    x_1, y_1, x_2, y_2 = int(x_1), int(y_1), int(x_2), int(y_2)\n",
    "    cv2.rectangle(image, (x_1, y_1), (x_2, y_2), (255, 0, 0), thickness=2, lineType = cv2.LINE_AA)\n",
    "    ((text_width, text_height), _) = cv2.getTextSize(category, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)    \n",
    "    cv2.rectangle(image, (x_1, y_1 - int(1.3 * text_height)), (x_1 + text_width, y_1), (0, 0, 0), -1)\n",
    "    cv2.putText(image, category, (x_1, y_1 - int(0.3 * text_height)), cv2.FONT_HERSHEY_SIMPLEX, 0.35, (255, 255, 255))\n",
    "    return image\n",
    "\n",
    "def visualize_bbox(image, bboxes, category_idxs, idx_to_cat):\n",
    "    img = image.copy()\n",
    "    for bbox, category_idx in zip(bboxes, category_idxs):\n",
    "        category = idx_to_cat[category_idx]\n",
    "        img = draw_bounding_boxes(img, bbox, category)\n",
    "    plt.figure(figsize = (8,8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "def extract_regions(img, scale = 100):\n",
    "    _, regions = selectivesearch.selective_search(img, scale = scale, min_size=100)\n",
    "    candidates = []\n",
    "    total_area = np.prod(img.shape[:2])\n",
    "    for region in regions:\n",
    "        if region['rect'] in candidates:\n",
    "            continue\n",
    "        # Remove rectangle which is less than 5% of total image area or greater than image\n",
    "        elif region['size'] < (0.05*total_area):\n",
    "            continue\n",
    "        elif region['size'] > (1*total_area):\n",
    "            continue\n",
    "        candidates.append(region['rect'])\n",
    "    return candidates\n",
    "\n",
    "def calculate_iou(box_1, box_2, epsilon = 1e-06):\n",
    "    iou = 0.0\n",
    "    box_1_area = (box_1[2] - box_1[0]) * (box_1[3] - box_1[1])\n",
    "    box_2_area = (box_2[2] - box_2[0]) * (box_2[3] - box_2[1])\n",
    "    x1_inter = max(box_1[0], box_2[0])\n",
    "    y1_inter = max(box_1[1], box_2[1])\n",
    "    x2_inter = min(box_1[2], box_2[2])\n",
    "    y2_inter = min(box_1[3], box_2[3])\n",
    "    inter_width = (x2_inter - x1_inter)\n",
    "    inter_height = (y2_inter - y1_inter)\n",
    "    if (inter_height < 0 or inter_width < 0):\n",
    "        return 0.0\n",
    "    inter_area = inter_width*inter_height\n",
    "    union_area = box_1_area - box_2_area - inter_area\n",
    "    iou = inter_area/(union_area + epsilon)\n",
    "    return iou"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>x_1</th>\n",
       "      <th>y_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>y_2</th>\n",
       "      <th>category_idx</th>\n",
       "      <th>category_name</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>000000000009.jpg</td>\n",
       "      <td>75</td>\n",
       "      <td>46</td>\n",
       "      <td>376</td>\n",
       "      <td>40</td>\n",
       "      <td>451</td>\n",
       "      <td>86</td>\n",
       "      <td>55</td>\n",
       "      <td>orange</td>\n",
       "      <td>data/coco/train2017/000000000009.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>000000000009.jpg</td>\n",
       "      <td>58</td>\n",
       "      <td>47</td>\n",
       "      <td>465</td>\n",
       "      <td>38</td>\n",
       "      <td>523</td>\n",
       "      <td>85</td>\n",
       "      <td>55</td>\n",
       "      <td>orange</td>\n",
       "      <td>data/coco/train2017/000000000009.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>000000000009.jpg</td>\n",
       "      <td>320</td>\n",
       "      <td>228</td>\n",
       "      <td>311</td>\n",
       "      <td>4</td>\n",
       "      <td>631</td>\n",
       "      <td>232</td>\n",
       "      <td>51</td>\n",
       "      <td>bowl</td>\n",
       "      <td>data/coco/train2017/000000000009.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>000000000009.jpg</td>\n",
       "      <td>316</td>\n",
       "      <td>245</td>\n",
       "      <td>249</td>\n",
       "      <td>229</td>\n",
       "      <td>565</td>\n",
       "      <td>474</td>\n",
       "      <td>56</td>\n",
       "      <td>broccoli</td>\n",
       "      <td>data/coco/train2017/000000000009.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>000000000009.jpg</td>\n",
       "      <td>94</td>\n",
       "      <td>71</td>\n",
       "      <td>364</td>\n",
       "      <td>2</td>\n",
       "      <td>458</td>\n",
       "      <td>73</td>\n",
       "      <td>55</td>\n",
       "      <td>orange</td>\n",
       "      <td>data/coco/train2017/000000000009.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id         file_name  width  height  x_1  y_1  x_2  y_2  \\\n",
       "0         9  000000000009.jpg     75      46  376   40  451   86   \n",
       "1         9  000000000009.jpg     58      47  465   38  523   85   \n",
       "2         9  000000000009.jpg    320     228  311    4  631  232   \n",
       "3         9  000000000009.jpg    316     245  249  229  565  474   \n",
       "4         9  000000000009.jpg     94      71  364    2  458   73   \n",
       "\n",
       "   category_idx category_name                                  path  \n",
       "0            55        orange  data/coco/train2017/000000000009.jpg  \n",
       "1            55        orange  data/coco/train2017/000000000009.jpg  \n",
       "2            51          bowl  data/coco/train2017/000000000009.jpg  \n",
       "3            56      broccoli  data/coco/train2017/000000000009.jpg  \n",
       "4            55        orange  data/coco/train2017/000000000009.jpg  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_folder = pathlib.Path('data/coco')\n",
    "train_image_folder = base_folder/'train2017'\n",
    "valid_image_folder = base_folder/'val2017'\n",
    "train_csv = pd.read_csv(base_folder/'train.csv')\n",
    "valid_csv = pd.read_csv(base_folder/'valid.csv')\n",
    "train_csv['path'] = list(map(lambda x:train_image_folder/x, train_csv['file_name']))\n",
    "# Transforming width and height column to width and height of the bounding box\n",
    "train_csv['width'] = train_csv['x_2'].values - train_csv['x_1'].values\n",
    "train_csv['height'] = train_csv['y_2'].values - train_csv['y_1'].values\n",
    "\n",
    "# Valid\n",
    "valid_csv['path'] = list(map(lambda x:valid_image_folder/x, valid_csv['file_name']))\n",
    "# Transforming width and height column to width and height of the bounding box\n",
    "valid_csv['width'] = valid_csv['x_2'].values - valid_csv['x_1'].values\n",
    "valid_csv['height'] = valid_csv['y_2'].values - valid_csv['y_1'].values\n",
    "train_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{64: 'potted plant', 13: 'stop sign', 49: 'knife', 67: 'dining table', 23: 'bear', 80: 'toaster', 87: 'scissors', 79: 'oven', 6: 'bus', 48: 'fork', 42: 'surfboard', 7: 'train', 22: 'elephant', 5: 'airplane', 90: 'toothbrush', 15: 'bench', 34: 'frisbee', 61: 'cake', 14: 'parking meter', 3: 'car', 44: 'bottle', 17: 'cat', 27: 'backpack', 59: 'pizza', 46: 'wine glass', 37: 'sports ball', 55: 'orange', 28: 'umbrella', 86: 'vase', 72: 'tv', 81: 'sink', 62: 'chair', 32: 'tie', 9: 'boat', 16: 'bird', 54: 'sandwich', 33: 'suitcase', 39: 'baseball bat', 19: 'horse', 77: 'cell phone', 18: 'dog', 40: 'baseball glove', 36: 'snowboard', 1: 'person', 65: 'bed', 10: 'traffic light', 50: 'spoon', 78: 'microwave', 2: 'bicycle', 24: 'zebra', 51: 'bowl', 25: 'giraffe', 43: 'tennis racket', 57: 'carrot', 76: 'keyboard', 11: 'fire hydrant', 84: 'book', 47: 'cup', 85: 'clock', 70: 'toilet', 75: 'remote', 82: 'refrigerator', 56: 'broccoli', 73: 'laptop', 74: 'mouse', 35: 'skis', 53: 'apple', 89: 'hair drier', 60: 'donut', 8: 'truck', 20: 'sheep', 21: 'cow', 58: 'hot dog', 4: 'motorcycle', 41: 'skateboard', 38: 'kite', 88: 'teddy bear', 63: 'couch', 52: 'banana', 31: 'handbag', 0: 'background'}\n"
     ]
    }
   ],
   "source": [
    "idx_to_cat = set(train_csv.groupby(['category_idx', 'category_name']).groups)\n",
    "idx_to_cat = dict(idx_to_cat)\n",
    "idx_to_cat[0] = 'background'\n",
    "cat_to_idx = {v:k for k,v in idx_to_cat.items()}\n",
    "print(idx_to_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCOData(Dataset):\n",
    "    def __init__(self, df, transforms = None):\n",
    "        self.df = df\n",
    "        self.unique_ids = df['image_id'].unique()\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.unique_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.unique_ids[idx]\n",
    "        df_sub = self.df[self.df['image_id'] == image_id]\n",
    "        # Since all path are same for subset\n",
    "        file_path = df_sub['path'].iloc[0]\n",
    "        image = cv2.imread(str(file_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        bboxes = df_sub[['x_1', 'y_1', 'width', 'height']].values\n",
    "        category_idx = df_sub['category_idx'].values.tolist()\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(image = image, bboxes = bboxes, category_idx=category_idx)\n",
    "            image = transformed['image']\n",
    "            bboxes = np.array(transformed['bboxes'])\n",
    "        H, W = image.shape[:2]\n",
    "        bboxes = np.hstack((bboxes[:, :2], bboxes[:, :2] + bboxes[:,2:]))\n",
    "        candidates = extract_regions(image)\n",
    "        candidates = np.array([(x, y, x+w, y+h) for x,y,w,h in candidates])\n",
    "        iou = np.array([[calculate_iou(bb, candidate) for bb in bboxes] for candidate in candidates])\n",
    "        current_lbls, current_offsets, img_patches = [], [], []\n",
    "        for idx, candidate in enumerate(candidates):\n",
    "            c_x_1, c_y_1, c_x_2, c_y_2 = candidate\n",
    "            img_patch = image[c_y_1: c_y_2, c_x_1:c_x_2]\n",
    "            img_patch = cv2.resize(img_patch, (224, 224))\n",
    "            img_patches.append(img_patch)\n",
    "            candidate_iou = iou[idx]\n",
    "            best_iou = np.argmax(candidate_iou)\n",
    "            iou_value = candidate_iou[best_iou]\n",
    "            bbox = bboxes[best_iou]\n",
    "            if iou_value > iou_threshold:\n",
    "                current_lbls.append(category_idx[best_iou])\n",
    "            else:\n",
    "                current_lbls.append(0)\n",
    "\n",
    "            # Make them in proportion to the image size\n",
    "            current_offset = np.array([bbox[0] - c_x_1, bbox[1] - c_y_1, bbox[2] - c_x_2, bbox[3] - c_y_2]) / np.array([W, H, W, H])\n",
    "            current_offsets.append(current_offset)\n",
    "        img_patches = torch.cat(img_patches)\n",
    "        current_lbls = torch.Tensor(current_lbls).long()\n",
    "        current_offsets = torch.Tensor(current_offsets).float()\n",
    "        return img_patches, current_lbls, current_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/thomas/Computer_Vision_DL/Day_19.ipynb Cell 12\u001b[0m in \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.129.131.104/home/thomas/Computer_Vision_DL/Day_19.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_transforms \u001b[39m=\u001b[39m A\u001b[39m.\u001b[39mCompose([A\u001b[39m.\u001b[39mHorizontalFlip(p \u001b[39m=\u001b[39m img_aug_p),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.129.131.104/home/thomas/Computer_Vision_DL/Day_19.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m                               A\u001b[39m.\u001b[39mShiftScaleRotate(p \u001b[39m=\u001b[39m img_aug_p),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.129.131.104/home/thomas/Computer_Vision_DL/Day_19.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m                               A\u001b[39m.\u001b[39mLongestMaxSize(max_size\u001b[39m=\u001b[39m img_resize_size),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.129.131.104/home/thomas/Computer_Vision_DL/Day_19.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m                               A\u001b[39m.\u001b[39mPadIfNeeded(img_resize_size, img_resize_size),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.129.131.104/home/thomas/Computer_Vision_DL/Day_19.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m                               A\u001b[39m.\u001b[39mRandomCrop(\u001b[39m*\u001b[39mimg_crop_size),\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.129.131.104/home/thomas/Computer_Vision_DL/Day_19.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m                               A\u001b[39m.\u001b[39mNormalize(mean, std, max_pixel_value\u001b[39m=\u001b[39m\u001b[39m255.0\u001b[39m),],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.129.131.104/home/thomas/Computer_Vision_DL/Day_19.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m                               bbox_params\u001b[39m=\u001b[39mA\u001b[39m.\u001b[39mBboxParams(\u001b[39m'\u001b[39m\u001b[39mcoco\u001b[39m\u001b[39m'\u001b[39m, label_fields\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mcategory_idx\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.129.131.104/home/thomas/Computer_Vision_DL/Day_19.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m test_transforms \u001b[39m=\u001b[39m A\u001b[39m.\u001b[39mCompose([A\u001b[39m.\u001b[39mLongestMaxSize(max_size\u001b[39m=\u001b[39m img_resize_size),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.129.131.104/home/thomas/Computer_Vision_DL/Day_19.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m                             A\u001b[39m.\u001b[39mRandomCrop(\u001b[39m*\u001b[39mimg_crop_size),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.129.131.104/home/thomas/Computer_Vision_DL/Day_19.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m                             A\u001b[39m.\u001b[39mNormalize(mean, std, max_pixel_value\u001b[39m=\u001b[39m\u001b[39m255.0\u001b[39m),],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.129.131.104/home/thomas/Computer_Vision_DL/Day_19.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m                             bbox_params\u001b[39m=\u001b[39mA\u001b[39m.\u001b[39mBboxParams(\u001b[39m'\u001b[39m\u001b[39mcoco\u001b[39m\u001b[39m'\u001b[39m, label_fields\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mcategory_idx\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.129.131.104/home/thomas/Computer_Vision_DL/Day_19.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m train_coco \u001b[39m=\u001b[39m COCOData(train_csv, train_transforms)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean' is not defined"
     ]
    }
   ],
   "source": [
    "train_transforms = A.Compose([A.HorizontalFlip(p = img_aug_p),\n",
    "                              A.ShiftScaleRotate(p = img_aug_p),\n",
    "                              A.LongestMaxSize(max_size= img_resize_size),\n",
    "                              A.PadIfNeeded(img_resize_size, img_resize_size),\n",
    "                              A.RandomCrop(*img_crop_size),\n",
    "                              A.Normalize(mean, std, max_pixel_value=255.0),],\n",
    "                              bbox_params=A.BboxParams('coco', label_fields=['category_idx']))\n",
    "\n",
    "test_transforms = A.Compose([A.LongestMaxSize(max_size= img_resize_size),\n",
    "                            A.RandomCrop(*img_crop_size),\n",
    "                            A.Normalize(mean, std, max_pixel_value=255.0),],\n",
    "                            bbox_params=A.BboxParams('coco', label_fields=['category_idx']))\n",
    "\n",
    "train_coco = COCOData(train_csv, train_transforms)\n",
    "valid_coco = COCOData(valid_csv, test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Train Images: 117266\n",
      "No of Test Images: 4952\n"
     ]
    }
   ],
   "source": [
    "print(f\"No of Train Images: {len(train_coco)}\")\n",
    "print(f\"No of Test Images: {len(valid_coco)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "986cada52ec410ee4ae5eaa595acb8590a81cbde4e49feb27784b90c9d35901c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
