{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE:__ For transforming the bboxes by albumentations library make sure you convert the bbox into 'coco' format which is [x_min, y_min, width, height], since it is the most stable. On repeated experimentation it was found that 'yolo' and 'pascal_voc' seems to be broken."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "import selectivesearch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import json\n",
    "import timm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_aug_p = 0.5\n",
    "iou_threshold = 0.4\n",
    "img_crop_size = (224, 224)\n",
    "img_resize_size = 256\n",
    "\n",
    "# mean and std-dev taken from imagenet dataset\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "batch_size = 2\n",
    "num_workers = 8\n",
    "\n",
    "invTransform = transforms.Compose([transforms.Normalize(mean = [0., 0., 0.], std = [1/std[0], 1/std[1], 1/std[2]]),\n",
    "                                  transforms.Normalize(mean = [-mean[0], -mean[1], -mean[2]], std = [1., 1., 1.])])\n",
    "\n",
    "model_name = 'resnet50'\n",
    "dropout_p = 0.4\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-04\n",
    "num_freeze = 100\n",
    "freeze = True\n",
    "lmb = 0.1\n",
    "\n",
    "save_path = './saved_models/rcnn_checkpoint.pt'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://albumentations.ai/docs/examples/example_bboxes/\n",
    "def draw_bounding_boxes(image, bbox, category):\n",
    "    x_1, y_1, x_2, y_2 = bbox\n",
    "    x_1, y_1, x_2, y_2 = int(x_1), int(y_1), int(x_2), int(y_2)\n",
    "    cv2.rectangle(image, (x_1, y_1), (x_2, y_2), (255, 0, 0), thickness=2, lineType = cv2.LINE_AA)\n",
    "    ((text_width, text_height), _) = cv2.getTextSize(category, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)    \n",
    "    cv2.rectangle(image, (x_1, y_1 - int(1.3 * text_height)), (x_1 + text_width, y_1), (0, 0, 0), -1)\n",
    "    cv2.putText(image, category, (x_1, y_1 - int(0.3 * text_height)), cv2.FONT_HERSHEY_SIMPLEX, 0.35, (255, 255, 255))\n",
    "    return image\n",
    "\n",
    "def visualize_bbox(image, bboxes, category_idxs, idx_to_cat):\n",
    "    img = image.copy()\n",
    "    for bbox, category_idx in zip(bboxes, category_idxs):\n",
    "        category = idx_to_cat[category_idx]\n",
    "        img = draw_bounding_boxes(img, bbox, category)\n",
    "    plt.figure(figsize = (8,8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "def extract_regions(img, scale = 100):\n",
    "    _, regions = selectivesearch.selective_search(img, scale = scale, min_size=100)\n",
    "    candidates = []\n",
    "    total_area = np.prod(img.shape[:2])\n",
    "    for region in regions:\n",
    "        if region['rect'] in candidates:\n",
    "            continue\n",
    "        # Remove rectangle which is less than 5% of total image area or greater than image\n",
    "        elif region['size'] < (0.05*total_area):\n",
    "            continue\n",
    "        elif region['size'] > (1*total_area):\n",
    "            continue\n",
    "        candidates.append(region['rect'])\n",
    "    return candidates\n",
    "\n",
    "def calculate_iou(box_1, box_2, epsilon = 1e-06):\n",
    "    iou = 0.0\n",
    "    box_1_area = (box_1[2] - box_1[0]) * (box_1[3] - box_1[1])\n",
    "    box_2_area = (box_2[2] - box_2[0]) * (box_2[3] - box_2[1])\n",
    "    x1_inter = max(box_1[0], box_2[0])\n",
    "    y1_inter = max(box_1[1], box_2[1])\n",
    "    x2_inter = min(box_1[2], box_2[2])\n",
    "    y2_inter = min(box_1[3], box_2[3])\n",
    "    inter_width = (x2_inter - x1_inter)\n",
    "    inter_height = (y2_inter - y1_inter)\n",
    "    if (inter_height < 0 or inter_width < 0):\n",
    "        return 0.0\n",
    "    inter_area = inter_width*inter_height\n",
    "    union_area = box_1_area - box_2_area - inter_area\n",
    "    iou = inter_area/(union_area + epsilon)\n",
    "    return iou\n",
    "\n",
    "def save_checkpoint(model, save_path):\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "def load_checkpoint(model, save_path):\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>x_1</th>\n",
       "      <th>y_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>y_2</th>\n",
       "      <th>category_idx</th>\n",
       "      <th>category_name</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>000000000009.jpg</td>\n",
       "      <td>75</td>\n",
       "      <td>46</td>\n",
       "      <td>376</td>\n",
       "      <td>40</td>\n",
       "      <td>451</td>\n",
       "      <td>86</td>\n",
       "      <td>55</td>\n",
       "      <td>orange</td>\n",
       "      <td>data/coco/train2017/000000000009.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>000000000009.jpg</td>\n",
       "      <td>58</td>\n",
       "      <td>47</td>\n",
       "      <td>465</td>\n",
       "      <td>38</td>\n",
       "      <td>523</td>\n",
       "      <td>85</td>\n",
       "      <td>55</td>\n",
       "      <td>orange</td>\n",
       "      <td>data/coco/train2017/000000000009.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>000000000009.jpg</td>\n",
       "      <td>320</td>\n",
       "      <td>228</td>\n",
       "      <td>311</td>\n",
       "      <td>4</td>\n",
       "      <td>631</td>\n",
       "      <td>232</td>\n",
       "      <td>51</td>\n",
       "      <td>bowl</td>\n",
       "      <td>data/coco/train2017/000000000009.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>000000000009.jpg</td>\n",
       "      <td>316</td>\n",
       "      <td>245</td>\n",
       "      <td>249</td>\n",
       "      <td>229</td>\n",
       "      <td>565</td>\n",
       "      <td>474</td>\n",
       "      <td>56</td>\n",
       "      <td>broccoli</td>\n",
       "      <td>data/coco/train2017/000000000009.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>000000000009.jpg</td>\n",
       "      <td>94</td>\n",
       "      <td>71</td>\n",
       "      <td>364</td>\n",
       "      <td>2</td>\n",
       "      <td>458</td>\n",
       "      <td>73</td>\n",
       "      <td>55</td>\n",
       "      <td>orange</td>\n",
       "      <td>data/coco/train2017/000000000009.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id         file_name  width  height  x_1  y_1  x_2  y_2  \\\n",
       "0         9  000000000009.jpg     75      46  376   40  451   86   \n",
       "1         9  000000000009.jpg     58      47  465   38  523   85   \n",
       "2         9  000000000009.jpg    320     228  311    4  631  232   \n",
       "3         9  000000000009.jpg    316     245  249  229  565  474   \n",
       "4         9  000000000009.jpg     94      71  364    2  458   73   \n",
       "\n",
       "   category_idx category_name                                  path  \n",
       "0            55        orange  data/coco/train2017/000000000009.jpg  \n",
       "1            55        orange  data/coco/train2017/000000000009.jpg  \n",
       "2            51          bowl  data/coco/train2017/000000000009.jpg  \n",
       "3            56      broccoli  data/coco/train2017/000000000009.jpg  \n",
       "4            55        orange  data/coco/train2017/000000000009.jpg  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_folder = pathlib.Path('data/coco')\n",
    "train_image_folder = base_folder/'train2017'\n",
    "valid_image_folder = base_folder/'val2017'\n",
    "train_csv = pd.read_csv(base_folder/'train.csv')\n",
    "valid_csv = pd.read_csv(base_folder/'valid.csv')\n",
    "train_csv['path'] = list(map(lambda x:train_image_folder/x, train_csv['file_name']))\n",
    "\n",
    "# Checking if the x_1 == x_2 or y_1 == y_2 then add +2 for some width or height\n",
    "train_csv.loc[train_csv['y_1'] == train_csv['y_2'], 'y_2'] = train_csv.loc[train_csv['y_1'] == train_csv['y_2'], 'y_2'] + 2\n",
    "train_csv.loc[train_csv['x_1'] == train_csv['x_2'], 'x_2'] = train_csv.loc[train_csv['x_1'] == train_csv['x_2'], 'x_2'] + 2\n",
    "\n",
    "# Transforming width and height column to width and height of the bounding box\n",
    "train_csv['width'] = train_csv['x_2'].values - train_csv['x_1'].values\n",
    "train_csv['height'] = train_csv['y_2'].values - train_csv['y_1'].values\n",
    "\n",
    "# Valid\n",
    "valid_csv['path'] = list(map(lambda x:valid_image_folder/x, valid_csv['file_name']))\n",
    "\n",
    "# Checking if the x_1 == x_2 or y_1 == y_2 then add +2 for some width or height\n",
    "valid_csv.loc[valid_csv['y_1'] == valid_csv['y_2'], 'y_2'] = valid_csv.loc[valid_csv['y_1'] == valid_csv['y_2'], 'y_2'] + 2\n",
    "valid_csv.loc[valid_csv['x_1'] == valid_csv['x_2'], 'x_2'] = valid_csv.loc[valid_csv['x_1'] == valid_csv['x_2'], 'x_2'] + 2\n",
    "\n",
    "# Transforming width and height column to width and height of the bounding box\n",
    "valid_csv['width'] = valid_csv['x_2'].values - valid_csv['x_1'].values\n",
    "valid_csv['height'] = valid_csv['y_2'].values - valid_csv['y_1'].values\n",
    "train_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_json = base_folder/'annotations/instances_train2017.json'\n",
    "with open(train_labels_json, 'r') as file:\n",
    "        data = json.load(file)\n",
    "idx_to_cat = {cat['id']:cat['name'] for cat in data['categories']}\n",
    "cat_to_idx = {cat['name']:cat['id'] for cat in data['categories']}\n",
    "\n",
    "# Some of the index are missing from data\n",
    "idx_not_present = []\n",
    "min_index = train_csv['category_idx'].min()\n",
    "max_index = train_csv['category_idx'].max()\n",
    "for i in range(min_index, max_index+1):\n",
    "    try:\n",
    "        _cat_name = idx_to_cat[i]\n",
    "    except:\n",
    "         idx_to_cat[i] = 'not_known'\n",
    "\n",
    "idx_to_cat[0] = 'background'\n",
    "cat_to_idx['background'] = 0\n",
    "print(len(idx_to_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCOData(Dataset):\n",
    "    def __init__(self, df, transforms = None, N = 1000):\n",
    "        self.df = df\n",
    "        self.unique_ids = df['image_id'].unique()\n",
    "        np.random.shuffle(self.unique_ids)\n",
    "        self.unique_ids = self.unique_ids[:N]\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.unique_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.unique_ids[idx]\n",
    "        df_sub = self.df[self.df['image_id'] == image_id]\n",
    "        # Since all path are same for subset\n",
    "        file_path = df_sub['path'].iloc[0]\n",
    "        image = cv2.imread(str(file_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        bboxes = df_sub[['x_1', 'y_1', 'width', 'height']].values\n",
    "        category_idx = df_sub['category_idx'].values.tolist()\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(image = image, bboxes = bboxes, category_idx=category_idx)\n",
    "            image = transformed['image']\n",
    "            bboxes = np.array(transformed['bboxes'])\n",
    "        H, W = image.shape[:2]\n",
    "        # If the bounding box is pushed outside the limits so there is no category only background\n",
    "        try:\n",
    "            bboxes = np.hstack((bboxes[:, :2], bboxes[:, :2] + bboxes[:,2:]))\n",
    "        except:\n",
    "            bboxes = np.array([[0, 0, 0, 0]])\n",
    "            category_idx = [0]\n",
    "\n",
    "        # Extract proposed regions and compute pairwise iou with all the bbox present\n",
    "        candidates = extract_regions(image)\n",
    "        candidates = np.array([(x, y, x+w, y+h) for x,y,w,h in candidates])\n",
    "        iou = np.array([[calculate_iou(bb, candidate) for bb in bboxes] for candidate in candidates])\n",
    "        current_lbls, current_offsets, img_patches = [], [], []\n",
    "\n",
    "        # Finding offset from the correct bounding boxes based on iou threshold\n",
    "        for idx, candidate in enumerate(candidates):\n",
    "            c_x_1, c_y_1, c_x_2, c_y_2 = candidate\n",
    "            img_patch = image[c_y_1: c_y_2, c_x_1:c_x_2]\n",
    "            img_patch = cv2.resize(img_patch, (224, 224))\n",
    "            img_patches.append(img_patch)\n",
    "            candidate_iou = iou[idx]\n",
    "            best_iou = np.argmax(candidate_iou)\n",
    "            iou_value = candidate_iou[best_iou]\n",
    "            bbox = bboxes[best_iou]\n",
    "            if iou_value > iou_threshold:\n",
    "                current_lbls.append(category_idx[best_iou])\n",
    "            else:\n",
    "                current_lbls.append(0)\n",
    "\n",
    "            # Make them in proportion to the image size\n",
    "            current_offset = np.array([bbox[0] - c_x_1, bbox[1] - c_y_1, bbox[2] - c_x_2, bbox[3] - c_y_2]) / np.array([W, H, W, H])\n",
    "            current_offsets.append(current_offset)\n",
    "\n",
    "        # Building Tensor out of Data\n",
    "        img_patches = np.array(img_patches)\n",
    "        current_lbls = np.array(current_lbls)\n",
    "        current_offsets = np.array(current_offsets)\n",
    "        img_patches = torch.Tensor(img_patches)\n",
    "        current_lbls = torch.Tensor(current_lbls).long()\n",
    "        current_offsets = torch.Tensor(current_offsets).float()\n",
    "        return img_patches, current_lbls, current_offsets\n",
    "    \n",
    "    def collate_function(self, batch):\n",
    "        # Since images may have multiple number of patches, this'll make them all concatenated together\n",
    "        final_inputs = []\n",
    "        final_lbls = []\n",
    "        final_offsets = []\n",
    "        for current_batch in batch:\n",
    "            final_inputs.append(current_batch[0])\n",
    "            final_lbls.append(current_batch[1])\n",
    "            final_offsets.append(current_batch[2])\n",
    "\n",
    "        final_inputs = torch.permute(torch.cat(final_inputs), (0, 3, 1, 2))\n",
    "        final_lbls = torch.cat(final_lbls)\n",
    "        final_offsets = torch.cat(final_offsets)\n",
    "        return final_inputs, final_lbls, final_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = A.Compose([A.HorizontalFlip(p = img_aug_p),\n",
    "                              A.ShiftScaleRotate(p = img_aug_p),\n",
    "                              A.LongestMaxSize(max_size= img_resize_size),\n",
    "                              A.PadIfNeeded(img_resize_size, img_resize_size),\n",
    "                              A.RandomCrop(*img_crop_size),\n",
    "                              A.Normalize(mean, std, max_pixel_value=255.0),],\n",
    "                              bbox_params=A.BboxParams('coco', label_fields=['category_idx']))\n",
    "\n",
    "valid_transforms = A.Compose([A.LongestMaxSize(max_size= img_resize_size),\n",
    "                              A.PadIfNeeded(img_resize_size, img_resize_size),\n",
    "                              A.RandomCrop(*img_crop_size),\n",
    "                              A.Normalize(mean, std, max_pixel_value=255.0),],\n",
    "                              bbox_params=A.BboxParams('coco', label_fields=['category_idx']))\n",
    "\n",
    "train_coco = COCOData(train_csv, train_transforms, N = 20000)\n",
    "valid_coco = COCOData(valid_csv, valid_transforms, N = -1)\n",
    "train_loader = DataLoader(train_coco, batch_size=batch_size, num_workers = num_workers, drop_last=True, \n",
    "                          collate_fn=train_coco.collate_function)\n",
    "valid_loader = DataLoader(valid_coco, batch_size=batch_size, num_workers=num_workers, \n",
    "                         collate_fn=valid_coco.collate_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"No of Train Images: {len(train_coco)}\")\n",
    "print(f\"No of Test Images: {len(valid_coco)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = next(iter(train_loader))\n",
    "idx = 7\n",
    "img = invTransform(temp[0][idx])\n",
    "img = torch.permute(img, (1, 2, 0))\n",
    "plt.imshow(img)\n",
    "plt.title(idx_to_cat[temp[1][idx].item()]);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNN(nn.Module):\n",
    "    def __init__(self, base_model = 'resnet50', lmb = 0.1, freeze = True,\n",
    "                 num_freeze = 100, dropout_p = 0.4, idx_to_cat = idx_to_cat):\n",
    "        super(RCNN, self).__init__()\n",
    "        model = timm.create_model(base_model, pretrained=True)\n",
    "        lin_features = model.num_features\n",
    "        layers = list(model.children())[:-1]\n",
    "        layers.extend([nn.Linear(lin_features, 512),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm1d(512),\n",
    "                        nn.Dropout(dropout_p),\n",
    "                        nn.Linear(512, 64),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm1d(64),\n",
    "                        nn.Dropout(dropout_p)])\n",
    "        \n",
    "        # Backbone architecture\n",
    "        self.feature_extractor = nn.Sequential(*layers)\n",
    "        if freeze:\n",
    "            self.freeze_layers(num_freeze)\n",
    "\n",
    "        # Classifier\n",
    "        self.categories = nn.Sequential(nn.Linear(64, len(idx_to_cat)))\n",
    "\n",
    "        # Offset prediction\n",
    "        self.offset = nn.Sequential(nn.Linear(64, 4),\n",
    "                                    nn.Tanh())\n",
    "\n",
    "        self.feature_extractor.apply(self.initialize_weights)\n",
    "        self.categories.apply(self.initialize_weights)\n",
    "        self.offset.apply(self.initialize_weights)\n",
    "\n",
    "        # Initializing loss and its parameters\n",
    "        self.lmb = lmb\n",
    "        self.l1 = nn.L1Loss()\n",
    "        self.cat_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    def initialize_weights(self, layer):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            torch.nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
    "            layer.bias.data.fill_(0.0)\n",
    "\n",
    "    def freeze_layers(self, num_freeze):\n",
    "        for idx, param in enumerate(self.feature_extractor.parameters()):\n",
    "            if idx <= num_freeze:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        cat = self.categories(features)\n",
    "        off = self.offset(features)\n",
    "        return cat, off\n",
    "    \n",
    "    def calculate_loss(self, cat_pred, offset_pred, cat_true, offset_true):\n",
    "        cat_loss = self.cat_entropy(cat_pred, cat_true)\n",
    "        idxs = torch.where(cat_true != 0)[0]\n",
    "        offset_pred = offset_pred[idxs]\n",
    "        offset_true = offset_true[idxs]\n",
    "        if len(idxs) > 0:\n",
    "            regression_loss = self.l1(offset_pred, offset_true)\n",
    "            final_loss = regression_loss*self.lmb + cat_loss\n",
    "            return final_loss, cat_loss.detach(), regression_loss.detach()\n",
    "        else:\n",
    "            final_loss = cat_loss\n",
    "            return final_loss, cat_loss.detach(), 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcnn = RCNN(base_model=model_name, lmb = lmb, freeze=freeze, num_freeze=num_freeze, dropout_p=dropout_p, idx_to_cat=idx_to_cat)\n",
    "rcnn = rcnn.to(device)\n",
    "optimizer = optim.Adam(rcnn.parameters(), lr = learning_rate)\n",
    "criterion = rcnn.calculate_loss\n",
    "summary(rcnn, (3, 224, 224));"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_of_correct(imgs, lbls, model):\n",
    "    imgs, lbls, model = imgs.to(device, non_blocking = True), lbls.to(device, non_blocking = True), model.to(device)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        out = model(imgs)[0]\n",
    "        _, preds= out.max(-1)\n",
    "        correct_preds = preds == lbls\n",
    "        return correct_preds.cpu().numpy().sum(), len(lbls)\n",
    "    \n",
    "def train_batch(imgs, lbls, offset_true, model, criterion, optimizer, scaler, valid = False):\n",
    "    optimizer.zero_grad()\n",
    "    imgs, lbls, model = imgs.to(device, non_blocking = True), lbls.to(device, non_blocking = True), model.to(device)\n",
    "    offset_true = offset_true.to(device, non_blocking = True)\n",
    "    with torch.cuda.amp.autocast():\n",
    "        out = model(imgs)\n",
    "        batch_loss, cat_loss, regression_loss = criterion(out[0], out[1], lbls, offset_true)\n",
    "    if not valid:\n",
    "        scaler.scale(batch_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    return batch_loss.item(), cat_loss, regression_loss\n",
    "\n",
    "def train(trainloader, validloader, model, criterion, optimizer, num_epochs, load_check = False,\n",
    "          best_valid_metric = 0.0, save_path = './saved_models/checkpoint.pt', early_stopping = True, epoch_threshold = 3):    \n",
    "    # Load From checkpoing\n",
    "    if load_check:\n",
    "        model = load_checkpoint(model, save_path)\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    train_accuracies, valid_accuracies = [], []\n",
    "    train_losses, valid_losses = [], []\n",
    "    no_change_epoch = 0\n",
    "    for epoch in range(num_epochs+1):\n",
    "        batch_loss = 0.0\n",
    "        model.train()\n",
    "        pbar = tqdm(trainloader, total = len(trainloader), leave = False)\n",
    "        for imgs, lbls_true, offset_true in pbar:\n",
    "            imgs = imgs.to(torch.float32)\n",
    "            current_batch_loss, current_reg_loss, current_cat_loss = train_batch(imgs, lbls_true, \n",
    "                                                                                 offset_true, model, criterion, optimizer, scaler)\n",
    "            batch_loss += current_batch_loss\n",
    "            pbar.set_postfix(train_loss = current_batch_loss)\n",
    "\n",
    "        total_correct = 0\n",
    "        total = 0\n",
    "        for imgs, lbls, _ in trainloader:\n",
    "            imgs = imgs.to(torch.float32)\n",
    "            batch_correct, batch_total = no_of_correct(imgs, lbls, model)\n",
    "            total_correct += batch_correct\n",
    "            total += batch_total\n",
    "\n",
    "        epoch_train_acc = total_correct/total\n",
    "        train_accuracies.append(epoch_train_acc)\n",
    "        epoch_train_loss = batch_loss / len(trainloader)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "\n",
    "        # Calculating for Valid data\n",
    "        batch_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total = 0\n",
    "        for imgs, lbls_true, offset_true in validloader:\n",
    "            imgs = imgs.to(torch.float32)\n",
    "            current_batch_loss, _, _ = train_batch(imgs, lbls_true, offset_true, model, criterion, optimizer, scaler, valid = True)\n",
    "            batch_loss += current_batch_loss\n",
    "            batch_correct, batch_total = no_of_correct(imgs, lbls_true, model)\n",
    "            total_correct += batch_correct\n",
    "            total += batch_total\n",
    "\n",
    "        epoch_valid_loss = batch_loss / len(validloader)\n",
    "        epoch_valid_acc = total_correct / total\n",
    "        valid_losses.append(epoch_valid_loss)\n",
    "        valid_accuracies.append(epoch_valid_acc)\n",
    "        \n",
    "        # Checkpoint\n",
    "        if epoch_valid_acc >= best_valid_metric:\n",
    "            print('#### Saving ####')\n",
    "            save_checkpoint(model, save_path)\n",
    "            best_valid_metric = epoch_valid_acc\n",
    "            # If the valid metric has changed set to -1 which will be made to 0, hence resetting the counter for early stopping\n",
    "            no_change_epoch = -1\n",
    "\n",
    "        no_change_epoch += 1\n",
    "        print(f\"{'*'*10} EPOCH {epoch:2}/{num_epochs} {'*'*10}\")\n",
    "        print(f'''{\"#\"*33}\n",
    "Train Loss: {epoch_train_loss:5.3f}, Train Accuracy: {epoch_train_acc*100:5.2f}\n",
    "Valid Loss: {epoch_valid_loss:5.3f}, Valid Accuracy: {epoch_valid_acc*100:5.2f}\n",
    "{\"#\"*33}''')\n",
    "        if no_change_epoch >= epoch_threshold:\n",
    "            break\n",
    "    return model, train_losses, train_accuracies, valid_losses, valid_accuracies, best_valid_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, train_losses, train_accuracies, valid_losses, valid_accuracies, best_valid_metric = train(train_loader, valid_loader, rcnn, criterion, optimizer, \n",
    "                                                                                                 num_epochs, save_path = save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "plt.subplot(121)\n",
    "plt.title('Accuracy vs Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(train_accuracies, 'r--', label = 'Train')\n",
    "plt.plot(valid_accuracies, 'g-', label = 'Valid')\n",
    "plt.legend()\n",
    "plt.subplot(122)\n",
    "plt.title('Losses vs Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(train_losses, 'r--', label = 'Train')\n",
    "plt.plot(valid_losses, 'g-', label = 'Valid')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze = False\n",
    "num_freeze = 0\n",
    "learning_rate = 1e-05\n",
    "rcnn = RCNN(base_model=model_name, lmb = lmb, freeze=freeze, num_freeze=num_freeze, dropout_p=dropout_p, idx_to_cat=idx_to_cat)\n",
    "rcnn = rcnn.to(device)\n",
    "optimizer = optim.Adam(rcnn.parameters(), lr = learning_rate)\n",
    "criterion = rcnn.calculate_loss\n",
    "\n",
    "model, train_losses, train_accuracies, valid_losses, valid_accuracies, best_valid_metric = train(train_loader, valid_loader, rcnn, criterion, optimizer, num_epochs, \n",
    "                                                                                                 load_check = True, best_valid_metric = best_valid_metric,\n",
    "                                                                                                 save_path = save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "plt.subplot(121)\n",
    "plt.title('Accuracy vs Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(train_accuracies, 'r--', label = 'Train')\n",
    "plt.plot(valid_accuracies, 'g-', label = 'Valid')\n",
    "plt.legend()\n",
    "plt.subplot(122)\n",
    "plt.title('Losses vs Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(train_losses, 'r--', label = 'Train')\n",
    "plt.plot(valid_losses, 'g-', label = 'Valid')\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "986cada52ec410ee4ae5eaa595acb8590a81cbde4e49feb27784b90c9d35901c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
